{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# NLP_LAB_4_LANGUAGE_COMPUTING\n","import nltk"],"metadata":{"id":"9upjBtFeEI11"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Task 1: Creation of N - gram Model"],"metadata":{"id":"BXxZPQBvC_7p"}},{"cell_type":"code","source":["ppnltk.download('punkt_tab')"],"metadata":{"id":"I_yLpzqeEMAx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"wB73NFfOUhjh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jqQoTRSoCwig"},"outputs":[],"source":["!pip install nltk scikit-learn"]},{"cell_type":"markdown","source":["\n","\n","\n","### Tasks to be performed\n","### (i) Tokenize the text using NLTK.\n","### (ii) Generate N-grams (unigrams, bigrams, trigrams) using nltk.util.ngrams().\n","### (iii) Use CountVectorizer from scikit-learn to extract n-grams automatically.\n","### (iv) Display the generated n-grams."],"metadata":{"id":"iSP6yFR1DgNh"}},{"cell_type":"code","source":["from nltk.util import ngrams\n","from collections import Counter\n","from sklearn.feature_extraction.text import CountVectorizer"],"metadata":{"id":"3ggyL8jODMwL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Take Sample text\n","text1 = 'Natural Language Processing enables machines to understand and process human language efficiently.'"],"metadata":{"id":"nj4lkH4tDVsV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenization\n","tokens1 = nltk.word_tokenize(text1.lower())  # Convert to lowercase for uniformity"],"metadata":{"id":"ylnpqsHMD3Nf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to generate N-grams\n","def generate_ngrams(tokens, n):\n","    return list(ngrams(tokens, n))"],"metadata":{"id":"30CeKOIkEbwr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate unigrams, bigrams, and trigrams\n","unigrams = generate_ngrams(tokens1, 1)\n","bigrams = generate_ngrams(tokens1, 2)\n","trigrams = generate_ngrams(tokens1, 3)"],"metadata":{"id":"iG0_4jH6EkLW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (i) Unigrams are individual words (single tokens).\n","### The function generate_ngrams(tokens, 1) extracts 1-word sequences from the tokenized text. Each word is treated as a single unit (1-tuple).\n","### (ii) Bigrams are sequences of two consecutive words.The function generate_ngrams(tokens, 2) extracts 2-word sequences. Each tuple represents a pair of adjacent words in the text.\n","### (iii) Trigrams are sequences of three consecutive words.The function generate_ngrams(tokens, 3) extracts 3-word sequences.Each tuple contains three consecutive words from the text."],"metadata":{"id":"Qq9IMDdyFE9n"}},{"cell_type":"code","source":["# Display the unigrams\n","unigrams = generate_ngrams(tokens1, 1)\n","print(\"Unigrams:\", unigrams)"],"metadata":{"id":"EbOe2GC8JFgB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Display the bigrams\n","bigrams = generate_ngrams(tokens1, 2)\n","print(\"\\nBigrams:\", bigrams)"],"metadata":{"id":"hH7uczYYEmmy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Display the trigrams\n","trigrams = generate_ngrams(tokens1, 3)\n","print(\"\\nTrigrams:\", trigrams)"],"metadata":{"id":"QzGtbOpUJcap"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use CountVectorizer for automatic N-gram extraction\n","vectorizer = CountVectorizer(ngram_range=(1, 3))\n"],"metadata":{"id":"wLHUgIHSEo7l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(vectorizer)"],"metadata":{"id":"WfbVH9ERZw01"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### CountVectorizer:\n","### It converts a given text into a matrix of token counts (Bag-of-Words model).\n","### ngram_range=(1, 3) specifies that we want to extract:Unigrams (1-grams), Bigrams (2-grams) and Trigrams (3-grams)\n","### This means the vectorizer will generate single words, two-word combinations, and three-word combinations."],"metadata":{"id":"5f0QrbWtGEHN"}},{"cell_type":"code","source":["X = vectorizer.fit_transform([text1.lower()])"],"metadata":{"id":"TOxBtg6pG-Iy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### text.lower(): Converts the text to lowercase to ensure uniformity.\n","### fit_transform([text.lower()]):\n","### fit(): Learns the vocabulary from the text.\n","### transform(): Converts the text into a numerical matrix (bag-of-words format) and returns a sparse matrix,\n","###where:\n","### Rows represent documents (since we have only one text, it's a single row).\n","### Columns represent extracted N-grams.\n","### Values represent the frequency of each N-gram in the text."],"metadata":{"id":"Nl_cGY7eGlat"}},{"cell_type":"code","source":["X"],"metadata":{"id":"9PEubkGXKpPv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","# Use CountVectorizer for automatic N-gram extraction\n","vectorizer1 = CountVectorizer(ngram_range=(1, 3))\n","X = vectorizer1.fit_transform([text1])\n","vectorizer.get_feature_names_out()\n","print(X.toarray())\n","vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(1,3))\n","X2 = vectorizer2.fit_transform([text1])\n","vectorizer2.get_feature_names_out()"],"metadata":{"id":"xFryw14Ctgpn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### get_feature_names_out():\n","### It returns a list of all extracted N-grams (unigrams, bigrams, and trigrams).\n","### The N-grams are arranged in alphabetical order."],"metadata":{"id":"kyo30ojcHEN7"}},{"cell_type":"markdown","source":["### Example 2: Create a N-gram Model for the Text paragraph taken as a list."],"metadata":{"id":"1dE-XWF9LDZb"}},{"cell_type":"code","source":["# Take Sample Paragraph text2 as a list as follows:\n","text2 =['Natural Language Processing enables machines to understand and process human language efficiently.','NLP has given a rich funtionalitites for symentic, syntax, and speech analysis.','Even Generative AI also used the buidling blocks of NLP']\n"],"metadata":{"id":"QWJ_WG55LN-8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Observe paragraph text2 is a string\n","type(text2)"],"metadata":{"id":"Mx1nrcdPirxH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Display the paragraph text2 senetnces\n","sentences=[]\n","for sentence in text2:\n","  print(sentence)\n","  sentences.append(sentence)"],"metadata":{"id":"TCfH8mVlMFGq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentences[0], sentences[1], sentences[2]"],"metadata":{"id":"VkfrO-s_PxKs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize, sent_tokenize"],"metadata":{"id":"FWqocvTx0O1x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenize words for each sentence in the list\n","tokens = []\n","for sentence in text2:\n","    tokens.extend(word_tokenize(sentence.lower()))  # Convert each sentence to lowercase"],"metadata":{"id":"3tvqb4qFzxe_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to generate N-grams\n","def generate_ngrams(tokens, n):\n","    return list(ngrams(tokens, n))"],"metadata":{"id":"uN-eOwR80BN_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate unigrams, bigrams, and trigrams\n","unigrams = generate_ngrams(tokens, 1)\n","bigrams = generate_ngrams(tokens, 2)\n","trigrams = generate_ngrams(tokens, 3)\n","\n","# Display results\n","print(\"Unigrams:\", unigrams)\n","print(\"\\nBigrams:\", bigrams)\n","print(\"\\nTrigrams:\", trigrams)"],"metadata":{"id":"TkzmVyUGz8tw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Using CountVectorizer for automatic N-gram extraction\n","vectorizer = CountVectorizer(ngram_range=(1, 3))\n","X = vectorizer.fit_transform([sentence.lower() for sentence in text2])\n","#X = vectorizer.fit_transform([text2.lower()])\n","ngrams_list = vectorizer.get_feature_names_out()\n","print(\"\\nGenerated N-grams using CountVectorizer:\")\n","print(ngrams_list)"],"metadata":{"id":"nxGgsl3t0XLi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","# Use CountVectorizer for automatic N-gram extraction\n","vectorizer = CountVectorizer(ngram_range=(1, 3))\n","X = vectorizer.fit_transform(text2)\n","vectorizer.get_feature_names_out()\n","print(X.toarray())\n","vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(1,3))\n","X2 = vectorizer2.fit_transform(text2)\n","vectorizer2.get_feature_names_out()"],"metadata":{"id":"Wdzb7PKog1-7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example 3:\n","from sklearn.feature_extraction.text import CountVectorizer\n","corpus=['A strong leader is essential for guiding a team towards its goals.',\n"," 'Effective leaders inspire and motivate their team members and provide clear direction and expectations, and create a positive and supportive work environment.',\n"," 'They empower their team members to take ownership of their work, encourage collaboration, and resolve conflicts constructively.',\n","'Leaders also play a crucial role in recognizing and celebrating the team achievements.',\n","'Finally fostering a sense of pride and accomplishment that fuels continued success.']\n","print(corpus)\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(corpus)\n","vectorizer.get_feature_names_out()\n","print(X.toarray())\n","vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(1, 1))\n","X2 = vectorizer2.fit_transform(corpus)\n","vectorizer2.get_feature_names_out()"],"metadata":{"id":"NrHuOfHtb3fc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Example 4:\n","from sklearn.feature_extraction.text import CountVectorizer\n","corpus = ['This is is the first document.','This document is the second document.', 'And this is the third one document.','Is this the first document?','Do we include the fifth one??']\n","print(corpus) # Vocabulary will consists of 9 unique words : {This, is, the, first, document, second, and, third, one}\n","vectorizer = CountVectorizer() # Each sentence will be represented in terms of 9 - vector\n","X = vectorizer.fit_transform(corpus)\n","vectorizer.get_feature_names_out()\n","print(X.toarray())\n","vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(1, 1))\n","X2 = vectorizer2.fit_transform(corpus)\n","vectorizer2.get_feature_names_out()"],"metadata":{"id":"s-24f981p38K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aKh8nr02ClGs"},"execution_count":null,"outputs":[]}]}