{"cells":[{"cell_type":"code","source":["# NLP_PRACTICAL_5_TEXT_PROCESSING_TASKS\n","! pip install nltk"],"metadata":{"id":"vfhBGMhEVNMQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I9qtzlaKfKDK"},"outputs":[],"source":["import nltk"]},{"cell_type":"code","source":["from nltk.corpus import gutenberg"],"metadata":{"id":"fo2vjtjmwR0t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('all')"],"metadata":{"id":"eBXvAQET3GDm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check whether the gutenberg corpus is uploaded\n","nltk.corpus.gutenberg"],"metadata":{"id":"uDI7asOZ1pvj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Determine the contents of gutenberg copora using fileids()\n","nltk.corpus.gutenberg.fileids()"],"metadata":{"id":"ImGy5kS53KUB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To know how many txt files in gutenberg\n","len(nltk.corpus.gutenberg.fileids())"],"metadata":{"id":"ffARNiLfAcYp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Open the three files for demonstration of text processing tasks and read the text using readLines()"],"metadata":{"id":"zSQiGRxA29no"}},{"cell_type":"code","source":["# Create the objects to access the contents of txt files present in gutenberg\n","blake = gutenberg.open('blake-poems.txt')\n","bible = gutenberg.open('bible-kjv.txt')\n","hamlet = gutenberg.open('shakespeare-hamlet.txt')\n"],"metadata":{"id":"j23jVQzm3urH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["blake=blake.readlines()\n","bible=bible.readlines()\n","hamlet=hamlet.readlines()"],"metadata":{"id":"vDTs6JgP3JzM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Know the no. of lines in each\n","len(blake), len(bible), len(hamlet)"],"metadata":{"id":"gShuUCr05Bry"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### We consider 'blake' for learning the text processing."],"metadata":{"id":"D8yREwFG1hh0"}},{"cell_type":"code","source":["blake"],"metadata":{"id":"GHEYUSn9xAnW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the no.of lines in blake\n","len(blake)"],"metadata":{"id":"H0KmKMWc2sJS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Select one of the text say blake, which consists of poems, songs...etc"],"metadata":{"id":"2CmJJYmqQorG"}},{"cell_type":"markdown","source":["### Task 1: Basic preprocessing - remove all the empty newlines in the corpus and strip any newline characters from other lines using strip()"],"metadata":{"id":"hickD1z76np5"}},{"cell_type":"code","source":["# To check for the blank lines in the text\n","for text in blake:\n","  print(text)\n","# Observe that len(blake) originally has no. of lines 1441 which includes balnk lines as well as empty strings"],"metadata":{"id":"dx6N8q2lSWN8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Observe the large no. of blank lines in the text. Use strip() to remove them"],"metadata":{"id":"AQ8gjjLWTO6V"}},{"cell_type":"markdown","source":["### strip('\\n') method is used to remove any newline characters (\\n) from the beginning and end of the string.\n","### text.strip('\\n') results in a new list where each element is a line from blake but without trailing newline characters."],"metadata":{"id":"cpl8hKX7T-rd"}},{"cell_type":"code","source":["for text in blake:\n","  print(text.strip('\\n'))"],"metadata":{"id":"z7ZkNqZJSwMo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### filter(None, ...) funciton applies the filter to the list created by the list comprehension.\n","### Argument None used in filter indicates it will remove any elements that evaluate to False in a boolean context. In Python, empty strings ('') are considered to be False.\n","### Therefore, this step effectively removes any blank lines (empty strings) from the list."],"metadata":{"id":"pceAT3G4UrKX"}},{"cell_type":"markdown","source":["### This is typically done in text processing to clean up the data and remove unnecessary whitespace or blank lines. By removing blank lines, it focuses the subsequent analysis on the actual content of the text. This is required for frequency analysis, tokenization, and other NLP operations."],"metadata":{"id":"FsM1SSdTVFdq"}},{"cell_type":"code","source":["blake = list(filter(None, [text.strip('\\n') for text in blake]))"],"metadata":{"id":"Jbor7sda68nm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Observe that blank lines have been removed but empty strings remain\n","blake[0:20]"],"metadata":{"id":"wPNq42Nof-Bu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(blake)"],"metadata":{"id":"JxQksdgjBm08"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Before strting with the next task let us know the length of each sentence(no. of characters in it)\n","for sentence in blake:\n","  print(sentence,len(sentence))"],"metadata":{"id":"PKPWFvvCgqi0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check length now after removing blank and \\n -\n","# It has been reduced\n","len(blake)\n","# Observe it is less than the original length of 1441"],"metadata":{"id":"jStF0CxUVetW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Try with other texts: bible, hamlet\n","hamlet = list(filter(None, [text.strip('\\n') for text in hamlet]))\n","bible = list(filter(None, [text.strip('\\n') for text in bible]))"],"metadata":{"id":"Gqg2JwCYCD-1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(blake),len(hamlet),len(bible)"],"metadata":{"id":"dhAj_u8NCQum"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Task 2: Basic frequency analysis on the corpus now.\n","### Task 2.1: Computing the length of each sentence and then visualize this using a histogram.\n","### Task 2.2: Visualize the overall distribution of typical sentence or line lengths across the file blake/hamlet/bible.\n"],"metadata":{"id":"KhuAlnvE7ys9"}},{"cell_type":"code","source":["# (2.1) Find the length of each sentence in the text - blake and make a list of all lengths\n","line_lengths_blake = [len(sentence) for sentence in blake]\n","print(line_lengths_blake)\n","# Observe that most of the sentences have around 17-42 characters and heighest no. of characters is from 2 - 8 ."],"metadata":{"id":"EWDxsFB67p6D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To visualize the overall distribution of typical sentence or line lengths across the text blake/hamlet/bible.\n","# import matplotlib.pyplot This line imports the `pyplot` module from the `matplotlib` library and assigns it the alias `plt`.\n","# This module provides functions for creating various types of plots, including histograms.\n","import matplotlib.pyplot as plt"],"metadata":{"id":"uSQC-VxPWgNb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Draw the histogram to visualize the distribution of words.\n","plt.hist(line_lengths_blake)"],"metadata":{"id":"yK1wUiJyuIVj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### line_lengths_blake is a variable which contains a list or array of numbers, where each number represents the length of a line in the 'blake' text.\n","### plt.hist() function from the matplotlib.pyplot module is used to create a histogram. It takes the data (line_lengths_blake in this case) as input and automatically bins the data into different intervals (or \"bins\").\n","### It shows you how many lines in the \"blake\" text are of different lengths. The taller bars represent the line lengths that occur more frequently and shorter bars for less frequent line lengths.\n","### The histogram plot has\n","### X-axis: Represents the line lengths (or binned intervals of line lengths).\n","### Y-axis: Represents the frequency or count of lines falling within each bin.\n","### Bars: The height of each bar indicates the number of lines with lengths falling within that particular bin."],"metadata":{"id":"WyJ7L_umaW0L"}},{"cell_type":"code","source":["h_blake = plt.hist(line_lengths_blake,color='green')\n","# color='green'` sets the color of the histogram bars to green."],"metadata":{"id":"XRn1hcxqbN4h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Text Analysis for bible\n","line_lengths_bible = [len(sentence) for sentence in bible]\n","print(line_lengths_bible)\n","h_bible = plt.hist(line_lengths_bible,color='yellow')"],"metadata":{"id":"fa9WaN5JHHXf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Task 2.3: Tokenize each sentence by splitting it into words and compute the length of each sentence to get the total words per sentence."],"metadata":{"id":"Eo66tHY_uV5d"}},{"cell_type":"code","source":["for sentence in blake:\n","  print(sentence.split(),len(sentence.split()))"],"metadata":{"id":"3Q_cdDxzcOqY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenize all the senetnces\n","lines_blake = [sentence.split() for sentence in blake]"],"metadata":{"id":"VQO6808rjbAw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(lines_blake)"],"metadata":{"id":"u15vXgjkiJQ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lines_blake[:5]"],"metadata":{"id":"-FzkinWGlZpO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lines_blake[0],lines_blake[1],lines_blake[2],lines_blake[3], lines_blake[4]"],"metadata":{"id":"rgy6DsbsHU4J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Draw histogram again\n","h_blake_new= plt.hist(lines_blake[0:1])"],"metadata":{"id":"DsRGVwNikDuR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens_per_line_blake = [len(sentence.split()) for sentence in blake]\n","print(tokens_per_line_blake)"],"metadata":{"id":"2OxUWIiJeK72"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(tokens_per_line_blake)"],"metadata":{"id":"aSVnQ6sDH1Ns"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Based on the 2nd visualization below, we conclude that most sentences in blake have roughly 3 - 8 words, or tokens."],"metadata":{"id":"ptxyykQFp5M7"}},{"cell_type":"code","source":["# Draw histogram again\n","h_blake_new= plt.hist(tokens_per_line_blake, color='orange')"],"metadata":{"id":"6vuUvZyJvnMu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenize all the senetnces\n","tokens_blake = [words.split() for words in blake]"],"metadata":{"id":"lteAjZ4Curpy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(tokens_blake)"],"metadata":{"id":"LgTnvXX9qKgA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens_blake[:5]"],"metadata":{"id":"MdzyRn-lqOoQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Task 3: To determine the most common words in the blake corpus. We already have our sentences tokenized into words (lists of words).\n","### Task 3.1: The first step involves flattening this big list of lists (each list is a tokenized sentence of words) into one big list of words.\n","### Observe that empty strings are eliminated automatically"],"metadata":{"id":"20tfjX86wYO3"}},{"cell_type":"code","source":["for sentence in tokens_blake:\n","  for word in sentence:\n","    print(word)"],"metadata":{"id":"YWfTNCVbquMP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract all the words and put it in a list\n","words_blake = [word for sentence in tokens_blake for word in sentence]"],"metadata":{"id":"pkL_t32zxLkp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(words_blake)"],"metadata":{"id":"ixreNXx8sKGm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(words_blake)"],"metadata":{"id":"lXaTKofSJF0C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(words_blake[:10])"],"metadata":{"id":"H3tttXqTxlBg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Task 3.2: Find the most frequent words invoke the counter from the collections - module"],"metadata":{"id":"UtZ-B9BBtLFe"}},{"cell_type":"code","source":["# To determine the most frequent words\n","from collections import Counter"],"metadata":{"id":"khTCpw6WzNku"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# convert all the owrds into lower case\n","words_blake = [word.lower() for word in words_blake]"],"metadata":{"id":"3P6aeSZmzVNc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A dictionary with jeys as words and frequencies as values is created when we use Counter()\n","c_blake = Counter(words_blake)"],"metadata":{"id":"UGLC2TVNzga1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dictionary is sorted by default as per the frequencies\n","print(c_blake)"],"metadata":{"id":"_12PtrYOtgM2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["c_blake.most_common(10)"],"metadata":{"id":"D4tVFcoAS1Bc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Task 4: To remove unwanted symbols and special characters in some of the words, use \"re.sub\""],"metadata":{"id":"h_7Ld_5wyBKa"}},{"cell_type":"code","source":["# Import the regular expressopn modeule\n","import re"],"metadata":{"id":"6M0j3HWgy2ni"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### re.sub() function is used for replacing occurrences of a pattern in a string with another string.\n","### It has three main arguments:\n","#### (1) pattern: The regular expression pattern to search for.\n","#### (2) replacement: The string to replace the matched pattern with.\n","##### (3) string: The input string on which the substitution is performed.\n","### where\n","### r'[^A-Za-z]' is the regular expression pattern.\n","#### r'' denotes a raw string, which is commonly used for regular expressions to avoid escaping special characters.\n","#### [^A-Za-z] is the actual pattern.\n","#### [] defines a character set.\n","#### ^ inside the character set means negation, i.e., match any character that is not in the set.\n","#### A-Za-z specifies all uppercase and lowercase letters, which helps to match any character that is not an alphabet (A-Z or a-z).\n","#### \"\": is the replacement string.\n","### In this case, it is an empty string. This means that any matched character (any non-alphabet) will be replaced with nothing, effectively removing it from the string.\n","#### word: This is the input string on which the substitution is applied. It represents the word that you want to process.\n"],"metadata":{"id":"yNDzo0Q9oBLq"}},{"cell_type":"code","source":["# Example\n","word=\"@ B. K. Birla College!!\"\n","re.sub(r'[^A-Za-z]', \"\", word)\n","# It has removed blank space, ., @, \"\", and !!"],"metadata":{"id":"kvvuyrmPpIyH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for word in words_blake:\n","  print(re.sub(r'[^A-Za-z]', \"\", word))"],"metadata":{"id":"-P4j2iTxuPjJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Actual removal of the blank spaces, and special charcters......."],"metadata":{"id":"B7AJBRT8LNC1"}},{"cell_type":"code","source":["words_blake = list(filter(None, [re.sub(r'[^A-Za-z]', \"\", word) for word in words_blake]))"],"metadata":{"id":"C_55k7axxuCo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(words_blake)"],"metadata":{"id":"OTWMNpx1zCxk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stopwords = nltk.corpus.stopwords.words('english')"],"metadata":{"id":"N8fa5hRJLMrt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stopwords"],"metadata":{"id":"qXckWeLELyi9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["words_blake = [word.lower() for word in words_blake if word.lower() not in stopwords]"],"metadata":{"id":"NJBuj8FqLqAY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["words_blake[:20]"],"metadata":{"id":"GUkDnZTAMNkQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["c_blake = Counter(words_blake)\n","c_blake.most_common(10)"],"metadata":{"id":"t5fEFUNEMgRb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Task 5: TEXT WRANGLING\n"],"metadata":{"id":"Q3iUFbSf2cTc"}},{"cell_type":"code","source":["# TASK 5.1: WEB SCRAPPING\n","# leverage requests and retrieve the contents of this web page in Python.This is known as web scraping"],"metadata":{"id":"kAJsN8mr2CQN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","data = requests.get('http://www.gutenberg.org/cache/epub/8001/pg8001.html')\n","content = data.content\n","print(content[1000:2200])"],"metadata":{"id":"ATHivbN72a6H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["content[0:1000]"],"metadata":{"id":"LRGSZRm6MfKd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###it is extremely difficult to decipher the actual textual content in the web page, due to all the unnecessary HTML tags.\n","### We need to remove those tags. The BeautifulSoup library provides functions that help us remove these unnecessary tags with ease"],"metadata":{"id":"T4rflfMu3vW1"}},{"cell_type":"code","source":["# Removign HTML Tags. Import re\n","import re"],"metadata":{"id":"NMR3sZc628my"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import BeautifulSoup from the bs4 library\n","from bs4 import BeautifulSoup"],"metadata":{"id":"jLTEfHrY4Ghf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Define the function for removing html tags"],"metadata":{"id":"r-l_6skOOcBQ"}},{"cell_type":"code","source":["def strip_html_tags(text):\n","  soup = BeautifulSoup(text, \"html.parser\")\n","  [s.extract() for s in soup(['iframe', 'script'])]\n","  stripped_text = soup.get_text()\n","  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n","  return stripped_text"],"metadata":{"id":"Auw3zQvT4LFu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clean_content = strip_html_tags(content)\n","print(clean_content[0:500])\n"],"metadata":{"id":"rcaBbuO_4zlU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Compare this output with the raw web page content and see that we have successfully removed the unnecessary HTML tags. We now have a clean body of text which is easier to interpret and understand."],"metadata":{"id":"VeITaL705LF3"}},{"cell_type":"code","source":["# TASK 5.2: TOKENIZATION : demonstartion of word and sentence tokenization for\n","# (i) inbuilt text (ii) sample text (iii) text from some other language"],"metadata":{"id":"MqQqdRzbEVk-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import gutenberg\n","from pprint import pprint\n","import numpy as np"],"metadata":{"id":"HCShQwN1EjOd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# (i) loading text corpora\n","alice = gutenberg.raw(fileids='carroll-alice.txt')"],"metadata":{"id":"qO7LdPj4E7Rm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To find total characters alice\n","print(len(alice))"],"metadata":{"id":"M1kKTmivF6FZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get first 100 characters\n","print(alice[0:100])"],"metadata":{"id":"i1-o9zf6GHL4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get defualt sentence tokenizer\n","sent_df=nltk.sent_tokenize()"],"metadata":{"id":"nZCzBVVIGpQR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(sent_df)"],"metadata":{"id":"8SSjf74kPahB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sent_df"],"metadata":{"id":"LPWZ4gl2PfZ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# extract senetnces from alice\n","alice_sentences=sent_df(text=alice)"],"metadata":{"id":"jamiIV9BG8FP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["alice_sentences[:50]"],"metadata":{"id":"GVYFENuhPvjk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Total sentences in alice text :',len(alice_sentences))\n","print('Sample alice text sentences :')\n","pprint(alice_sentences[0:5])\n"],"metadata":{"id":"k1vDS-h5QA9A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# (ii) Take some sample text\n","sample_text='We will discuss briefly about the basic syntax,structure and design philosphies. There is defined hierrachical syntax for Python code which you should remember when writing code! Python is really powerful programming language !'"],"metadata":{"id":"bPQRp1gQPPGu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(sample_text)"],"metadata":{"id":"X-zGz0LxQSgT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokenizing with default toenizer 'sent_df'\n","sample_sentences=sent_df(text=sample_text)"],"metadata":{"id":"-P6IJmnwHYfl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Total sentences in sample text :',len(sample_sentences))\n","print('Sample text sentences :')\n","pprint(sample_sentences)"],"metadata":{"id":"UvwW0STLHue9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# (iii) Processing some other langauge text say German\n","from nltk.corpus import europarl_raw"],"metadata":{"id":"nM_wetfFJ8dG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('all')"],"metadata":{"id":"GekgsFUPKqNP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.corpus.europarl_raw"],"metadata":{"id":"VP3eIThjKx3T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["german_text=europarl_raw.german.raw(fileids='ep-00-01-17.de')"],"metadata":{"id":"qWvY6iBwKQAe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(german_text))"],"metadata":{"id":"vJbL99X7LdOE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(german_text[0:100])"],"metadata":{"id":"zjamCB06LmLm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["german_sentences_def=sent_df(text=german_text,language='german')"],"metadata":{"id":"q_LDG-v7L1EW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["german_sentences_def"],"metadata":{"id":"d8lHBuAsRxCn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# loading the german text tokenizer\n","german_tokenizer=nltk.data.load(resource_url='tokenizers/punkt/german.pickle')"],"metadata":{"id":"EV1s3gdoMMmi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To verify the invoking of pretrained tokenizer\n","print (type(german_tokenizer))"],"metadata":{"id":"vhU4tHLiNCsj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply the pretrained tokenizer on the german_text\n","german_sentences=german_tokenizer.tokenize(german_text)"],"metadata":{"id":"FrdpO3RhMu24"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["german_sentences"],"metadata":{"id":"Fx0qaBZCSMDB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To verify whether the sentences obtained from the default tokenizer 'sent_df'\n","# are same as the sentences obtained from the pre-trained tokenizer\n","print(german_sentences_def==german_sentences)"],"metadata":{"id":"dJ6P3Kb2MEGi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for sent in german_sentences[0:10]:\n","  print(sent)"],"metadata":{"id":"XiXGqhWmPY5B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Verify for the sample text generated above"],"metadata":{"id":"exeZsiTdSuES"}},{"cell_type":"code","source":["# tokenizing sample text with default toenizer 'sent_df'\n","sample_sentences_def=sent_df(text=sample_text)"],"metadata":{"id":"LB6QtZ6sN0T8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokenizing sample text with pre-trained tokenizer :PunktSentenceTokenizer()\n","punkt_pretrained=nltk.tokenize.PunktSentenceTokenizer()"],"metadata":{"id":"5lf_ND0HQeQB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_sentences=punkt_pretrained.tokenize(sample_text)"],"metadata":{"id":"-L7CDDttRBCl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To verify\n","print(sample_sentences_def==sample_sentences)"],"metadata":{"id":"AOqzKcohRSEw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pprint(sample_sentences)"],"metadata":{"id":"6qX0ni5xRgbF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Task 5.3 Word Tokenization verfication for both default and pre-trained Word Tokenizer(WT)"],"metadata":{"id":"KHtca90BRuSS"}},{"cell_type":"code","source":["sentence=\"The brown fox wasn't quick and he couldn't win the race\""],"metadata":{"id":"hXF98Rv0RtOD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Invoke default word tokenizer (wt)\n","default_wt=nltk.word_tokenize"],"metadata":{"id":"Kuiy4ezaSIRk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create words through default wt\n","words_df=default_wt(sentence)\n","print(words_df)"],"metadata":{"id":"73_b7_3zSxiS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Using Pre-trained Tokenizer\n","#### TreebankWordTokenizer is a specific class within NLTK that provides a word tokenizer trained on the Penn Treebank corpus. The Penn Treebank is a large annotated corpus of English text commonly used in natural language processing research.\n","#### Here we can define\n","#### treebank_wt, a variable that will hold the instance of the TreebankWordTokenizer class and use this variable to tokenize text using the Treebank tokenizer.\n"],"metadata":{"id":"r9vUAZrsj9YG"}},{"cell_type":"code","source":["# Use the pre- trained tokenizer say TreebankTokenizer\n","treebank_wt=nltk.TreebankWordTokenizer()"],"metadata":{"id":"sL2BpWASS-dG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create words using treebank_wt\n","words_treebank=treebank_wt.tokenize(sentence)\n","print(words_treebank)"],"metadata":{"id":"iWUgWckwTpmT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To verify the words of default and pretrained tokenizer\n","print(words_df==words_treebank)"],"metadata":{"id":"-WZEB1kI3wH9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To remove the white spaces say space between could and n't\n","whitespaces_wt=nltk.WhitespaceTokenizer()"],"metadata":{"id":"xQ0IMpE9T9na"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create words using whitespaces_wt\n","words_white=whitespaces_wt.tokenize(sentence)\n","print(words_white)"],"metadata":{"id":"2pb6MvqsU0KZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Task 6: Text Normalization"],"metadata":{"id":"ybJdbXkcVVR3"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","import re\n","import string\n","from pprint import pprint"],"metadata":{"id":"HSGUZgBmVUpy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating our own corpus\n","my_corpus=[\"The brown fox wasn't quick and he couldn't win the race.\",\n","\"Hey it was a great cricket match @yesterday!!\",\n","\"I just bought a @new mobile for me at $1000.\",\n","\"Python NLP is really ****amazing*****!!@@@\"]"],"metadata":{"id":"1LPTtV14VsOX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(my_corpus)"],"metadata":{"id":"cjJD7AAKe0Pb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_corpus[0], my_corpus[1], my_corpus[2], my_corpus[3]"],"metadata":{"id":"NOsWdt405H38"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenize the my_corpus\n","def tokenize_text(text):\n","  sentences=nltk.sent_tokenize(text)\n","  word_tokens=[nltk.word_tokenize(sentence) for sentence in sentences]\n","  return word_tokens"],"metadata":{"id":"M3xquDAPWzHr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the token list from the above def\n","token_list=[tokenize_text(text) for text in my_corpus]\n","print(token_list)"],"metadata":{"id":"iJzoGPFRWjQE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Take the example of you choice and demonstrate the removal Special  Characters\n","### An important task in text normalization involves removing unnecessary and special characters.\n","### These may be special symbols or even punctuation that occurs in sentences.\n","### This step is often performed before or after tokenization. The main reason for doing so is because often punctuation or special characters do not have much significance when we analyze the text and utilize it for extracting features or information based on NLP and ML. It is possible to implement both types of special characters removal, before and after tokenization."],"metadata":{"id":"yZ6G60t_TZnA"}},{"cell_type":"code","source":["# Task 5.3: Removing special characters for any given sentence\n","def remove_special_characters(text):# remove_digits = False is optional\n","  pattern = r'[^a-zA-z0-9\\s]'\n","  #if not remove_digits else r'[^a-zA-z\\s]'\n","  text = re.sub(pattern,'', text)\n","  return text"],"metadata":{"id":"VvNE1TOLZrPc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["remove_special_characters(\"I just bought a @new mobile for me at $1000 dollars\\n\"\n","\"Python NLP is really ****amazing*****!!@@@\")"],"metadata":{"id":"PzNZMnwaZwPc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["remove_special_characters('Mary had a @@@ little lamb !!#$%&*')"],"metadata":{"id":"47WiRhu98aEV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Removing special characters before and after tokenization\n","import re\n","import string\n","from pprint import pprint"],"metadata":{"id":"sPptfy4xZqsP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To remove special characters  after  tokenization use re.compile, re.escape and format\n","def remove_characters_after_tokenization(tokens):\n","    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n","    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n","    return filtered_tokens"],"metadata":{"id":"tJrKVC6ATKQK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### my_corpus=[\"The brown fox wasn't quick and he couldn't win the race.\",\"Hey it was a great cricket match @yesterday!!\", \"I just bought a @new mobile for me at $1000.\", \"Python NLP is really ****amazing*****!!@@@\"]"],"metadata":{"id":"McB57khk-38d"}},{"cell_type":"code","source":["filtered_list_1=[list(filter(None,[list(remove_characters_after_tokenization(tokens))for tokens in sentence_tokens]))for sentence_tokens in token_list]\n","#token_list=[tokenize_text(text) for text in my_corpus]\n","print(filtered_list_1)"],"metadata":{"id":"hoCi6dhXUuO5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Removing special characters before tokenization\n","def remove_characters_before_tokenization(sentence,keep_apostrophes=False):\n","  sentence = sentence.strip()\n","  if keep_apostrophes:\n","    PATTERN = r'[?|$|&|*|%|@|(|)|~]' # add other characters here to remove them\n","    filtered_sentence = re.sub(PATTERN, r'', sentence)\n","  else:\n","    PATTERN = r'[^a-zA-Z0-9 ]' # only extract alpha-numeric characters\n","  filtered_sentence = re.sub(PATTERN, r'', sentence)\n","  return filtered_sentence"],"metadata":{"id":"OCGFT4e3e-Pm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filtered_list_2 = [remove_characters_before_tokenization(sentence)\n","for sentence in my_corpus]\n","pprint(filtered_list_2)"],"metadata":{"id":"MiID5DoOfX3V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cleaned_my_corpus=[remove_characters_before_tokenization(sentence,keep_apostrophes=True)\n","for sentence in my_corpus]\n","pprint(cleaned_my_corpus)"],"metadata":{"id":"Okj8SLoHgIZm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install contractions"],"metadata":{"id":"5uGmcBg58wjD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import contractions\n","import re"],"metadata":{"id":"abcPijmX89IL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["contraction_mapping=contractions.contractions_dict\n","print(contraction_mapping)"],"metadata":{"id":"JzP_HGVT6ofl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Replacing contractions with expanded form\n","def expand_contractions(sentence, contraction_mapping):\n","  contractions_pattern=re.compile('({})'.format('|'.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL)\n","  expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n","  return expanded_sentence\n","\n","# Matching the contraction in the sentence and reaplcing it with its expanded form\n","def expand_match(contraction):\n","  match = contraction.group(0)\n","  #first_char = match[0]\n","  expanded_contraction = contraction_mapping.get(match)\n","  if expanded_contraction:\n","    return expanded_contraction\n","  else:\n","    return contraction_mapping.get(match.lower())\n","    #expanded_contraction = first_char + expanded_contraction[1:]\n","contraction_mapping = contractions.contractions_dict  # Assuming 'contractions' is already imported\n","sentence = \"\"\n","# Define contractions_pattern outside the function\n","contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), flags=re.IGNORECASE | re.DOTALL)\n","expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n","pprint(expanded_sentence)"],"metadata":{"id":"3U-iq6CIEwvA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["expanded_my_corpus=[expand_contractions(sentence,contraction_mapping) for sentence in cleaned_my_corpus]\n","sentences = str(cleaned_my_corpus)\n","for contraction, expanded_form in contraction_mapping.items():\n","    sentences = sentences.replace(contraction, expanded_form)\n","# Print the expanded sentence\n","pprint(sentences)"],"metadata":{"id":"CNUNFy_O_3LR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# alternate definition of expand_match()\n","def expand_match(contraction):\n","    match = contraction.group(0)\n","    expanded_contraction = contraction_mapping.get(match)\n","    if expanded_contraction:\n","        return expanded_contraction\n","    else:\n","        return contraction_mapping.get(match.lower())\n","    return expanded_contraction"],"metadata":{"id":"f0mYP5TbN-KU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentence=\"I wasn't getting the right code.\"\n","for contraction, expanded_form in contraction_mapping.items():\n","    sentence = sentence.replace(contraction, expanded_form)\n","# Print the expanded sentence\n","print(sentence)"],"metadata":{"id":"ynj4ECCqOcve"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["expanded_my_corpus=[expand_contractions(sentence,contraction_mapping) for sentence in cleaned_my_corpus]\n","print(expanded_my_corpus)"],"metadata":{"id":"G1obxQG2OQQo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Case Conversion"],"metadata":{"id":"gKE4sPFUhVY2"}},{"cell_type":"code","source":["my_corpus[0]"],"metadata":{"id":"czbG0KaJhbTi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(my_corpus[0].lower())"],"metadata":{"id":"B9F7XknohYO4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(my_corpus[0].upper())"],"metadata":{"id":"_FOCdxJThmks"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Removing Stopwords Stopwords   \n","# Stopwords are words that have little or no significance.\n","# They are usually removed from text during processing so as to retain words having maximum significance and context.\n","# Stopwords are usually words that end up occurring the most if you aggregated any corpus of text based on singular tokens and checked their frequencies.\n","# Words like  a, the ,  me , and so on are stopwords.\n","# There is no universal or exhaustive list of stopwords.Each domain or language may have its own set of stopwords."],"metadata":{"id":"MaaLMSmIil4K"}},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')"],"metadata":{"id":"Ibf35e2Rm4Ji"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.tokenize.toktok import ToktokTokenizer"],"metadata":{"id":"VWPOXDjfmGi_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = ToktokTokenizer()\n","stopword_list = nltk.corpus.stopwords.words('english')\n","print(stopword_list[:10])"],"metadata":{"id":"PmDN_E7DmKJi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def remove_stopwords(tokens):\n","  stopword_list = nltk.corpus.stopwords.words('english')\n","  filtered_tokens = [token for token in tokens if token not in stopword_list]\n","  return filtered_tokens\n","  if is_lower_case:\n","    filtered_tokens = [token for token in tokens if token not in stopword_list]\n","  else:\n","    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n","  filtered_text=' '.join(filtered_tokens)\n","  return filtered_text"],"metadata":{"id":"Gb5C3xYUilQP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["remove_stopwords(\"The, and, if are stopwords, computer is not\")"],"metadata":{"id":"NtHyaWIEnc0s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Correcting Words"],"metadata":{"id":"8EyObdMFuNo8"}},{"cell_type":"code","source":["import nltk"],"metadata":{"id":"suLfuv8PvQiD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('wordnet')"],"metadata":{"id":"4nUNA-U8vczO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.corpus import wordnet"],"metadata":{"id":"9mZnsKleuWM0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wordnet.fileids()"],"metadata":{"id":"DTZ8-9BSw78Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["old_word='finalllyyy'\n","repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n","match_substitution = r'\\1\\2\\3'\n","step = 1\n","while True:\n","  # check for semantically correct word\n","  if wordnet.synsets(old_word):\n","    print('Final correct word: ',old_word)\n","  break\n","# remove one repeated character\n","  new_word = repeat_pattern.sub(match_substitution,old_word)\n","  if new_word != old_word:\n","    print('Step: {} Word: {}'.format(step, new_word))\n","    step=step+1\n","    # update step and update old word to last substituted state\n","    old_word = new_word\n","    continue\n","  else:\n","    print('Final word: ', new_word)\n","  break"],"metadata":{"id":"4cNsO2KxkAYR"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1iSMNNBUrGW_8WNlJLw8SUN3IStQEpuW_","timestamp":1711171236304}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}